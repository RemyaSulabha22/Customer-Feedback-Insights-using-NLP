# -*- coding: utf-8 -*-
"""Capstone Try .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbP3qTopz275-jVXssBLvV1RXLvXb1u9
"""

# Commented out IPython magic to ensure Python compatibility.

import numpy as np

# For Panel Data Analysis
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('mode.chained_assignment', None)
pd.set_option('display.precision', 4)

#from pandas_profiling import ProfileReport

# For Data Visualization
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

# To Disable Warnings
import warnings
warnings.filterwarnings("ignore")

import re
import nltk

nltk.download('all')

from wordcloud import WordCloud

from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report



import pandas as pd

# Load the dataset

reviews_df = pd.read_csv('https://storage.googleapis.com/retail-analytics-data/reviews_us_Electronics_v1_00.tsv', sep='\t', quoting=3)
reviews_df.head(3)

reviews_df.describe()

reviews_df['review_body'] = reviews_df['review_body'].astype(str)

reviews_df['review_body'] = reviews_df['review_body'].astype(str)

# Creating a helper function to clean the text.
def clean_text(text):

    text = text.lower().strip()
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"how's", "how is", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "cannot", text)
    text = re.sub(r"n't", " not", text)
    text = re.sub(r"<br>", " ", text)

    #Complex Expression
    text = re.sub(r"([-?.!,/\"])", r" \1 ", text)
    text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,']", "", text)
    text = re.sub(r"[ ]+", " ", text)
    text = text.rstrip().strip()

    return text

re.sub(r"([-!?.,/\"])", r" \1 ", 'Hi!')

re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,']", "", 'Hi ! ')

re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,']", "", 'Hi ! ')

reviews_df['clean_reviews'] = reviews_df['review_body'].apply(lambda x:clean_text(x))
reviews_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# polarity = []
# subjectivity = []
# 
# for review in reviews_df['clean_reviews']:
#     blob = TextBlob(review)
#     polarity.append(blob.sentiment.polarity)
#     subjectivity.append(blob.sentiment.subjectivity)

reviews_df['polarity'] = polarity
reviews_df['subjectivity'] = subjectivity



!pip install datasets

!pip install evaluate

#For train-test split and evaluation
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# For BERT Fine-tuning
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import evaluate

# Load the accuracy metric
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Display basic info
print("Dataset shape:", reviews_df.shape)
print(reviews_df.head(3))

# Ensure review_body is string (if column exists; if not, adjust column name)
reviews_df['review_body'] = reviews_df['review_body'].astype(str)

# Define text cleaning function
def clean_text(text):
    text = text.lower().strip()
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"how's", "how is", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "cannot", text)
    text = re.sub(r"n't", " not", text)
    text = re.sub(r"<br>", " ", text)
    text = re.sub(r"([-?.!,/\"])", r" \1 ", text)
    text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,']", "", text)
    text = re.sub(r"[ ]+", " ", text)
    return text

# Apply cleaning to review_body
reviews_df['clean_reviews'] = reviews_df['review_body'].apply(clean_text)

# Compute polarity and subjectivity using TextBlob
def compute_sentiments(text):
    blob = TextBlob(text)
    return pd.Series([blob.sentiment.polarity, blob.sentiment.subjectivity])

reviews_df[['polarity', 'subjectivity']] = reviews_df['clean_reviews'].apply(compute_sentiments)

# Define a function to classify sentiment into 4 categories:
# positive, negative, neutral, and sarcasm.
def classify_sentiment(polarity, subjectivity, text):
    # Define a list of keywords often used sarcastically
    sarcasm_keywords = ["oh", "great", "just what", "sure", "amazing", "fantastic", "wow"]
    if subjectivity > 0.7 and polarity < 0.1 and any(word in text for word in sarcasm_keywords):
        return "sarcasm"
    elif polarity > 0.3:
        return "positive"
    elif polarity < -0.3:
        return "negative"
    else:
        return "neutral"

reviews_df['sentiment'] = reviews_df.apply(lambda row: classify_sentiment(row['polarity'], row['subjectivity'], row['clean_reviews']), axis=1)

print("\nSentiment distribution:")
print(reviews_df['sentiment'].value_counts())

# 3. Exploratory Data Analysis (EDA) & Visualizations
# ================================
# Plot sentiment distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='sentiment', data=reviews_df, order=["positive", "negative", "neutral", "sarcasm"])
plt.title("Sentiment Distribution")
plt.show()

# Histogram of polarity and subjectivity
fig, axs = plt.subplots(1, 2, figsize=(12, 5))
sns.histplot(reviews_df['polarity'], bins=30, ax=axs[0])
axs[0].set_title("Polarity Distribution")
sns.histplot(reviews_df['subjectivity'], bins=30, ax=axs[1])
axs[1].set_title("Subjectivity Distribution")
plt.show()

# Correlation heatmap between polarity and subjectivity
plt.figure(figsize=(5, 4))
corr = reviews_df[['polarity', 'subjectivity']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Correlation between Polarity and Subjectivity")
plt.show()

# Word Clouds for each sentiment (using a sample of reviews)
for sentiment in ["positive", "negative", "neutral", "sarcasm"]:
    subset = reviews_df[reviews_df['sentiment'] == sentiment]
    text = " ".join(subset['clean_reviews'].tolist()[:1000])  # sample subset for word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {sentiment.capitalize()} Sentiment")
    plt.show()

# ================================
# 4. Data Post Processing for Modeling
# ================================
# Create label mapping for sentiment classes
label_map = {"negative": 0, "neutral": 1, "positive": 2, "sarcasm": 3}
reviews_df['label'] = reviews_df['sentiment'].map(label_map)

# Prepare data for modeling
X = reviews_df['clean_reviews']
y = reviews_df['label']

# Split the data into training and testing sets (stratified)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print("\nTraining samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

# ================================
# 5. Model 1: TF-IDF + Naïve Bayes
# ================================
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('nb', MultinomialNB())
])

# Train the TF-IDF + Naïve Bayes model
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

print("\nTF-IDF + Naïve Bayes Classification Report:")
print(classification_report(y_test, y_pred, target_names=list(label_map.keys())))
print("TF-IDF + NB Accuracy:", accuracy_score(y_test, y_pred))

# ================================

# 6. Model 2: BERT Fine-Tuning for 4-Class Sentiment Classification
# ================================
# Create DataFrames for Hugging Face Datasets
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
test_df  = pd.DataFrame({'text': X_test, 'label': y_test})

# Create Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset  = Dataset.from_pandas(test_df)

# Load the tokenizer and model
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch tensors
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Load BERT model for sequence classification (4 classes)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4)

#!pip install --upgrade transformers

# 6. Model 2: BERT Fine-Tuning for 4-Class Sentiment Classification
# ================================
# Create DataFrames for Hugging Face Datasets
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
test_df  = pd.DataFrame({'text': X_test, 'label': y_test})

# Map sentiment labels to numeric values
label_map = {"negative": 0, "neutral": 1, "positive": 2, "sarcasm": 3}
reviews_df['label'] = reviews_df['sentiment'].map(label_map)

# Prepare data for modeling
X = reviews_df['clean_reviews']
y = reviews_df['label']

# Split data into training and testing sets (stratified)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

# 6. Model 2: BERT Fine-Tuning for 4-Class Sentiment Classification
# ================================
# Create DataFrames for Hugging Face Datasets
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
test_df  = pd.DataFrame({'text': X_test, 'label': y_test})

train_dataset = Dataset.from_pandas(train_df)
test_dataset  = Dataset.from_pandas(test_df)

model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4)
# Define training arguments (simplified to avoid ValueError)
training_args = TrainingArguments(
    output_dir="bert_sentiment",
    #evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,   # Increase if needed
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    push_to_hub=False
)

# Load the accuracy metric using evaluate
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

# Train the BERT model
trainer.train()

# Evaluate the BERT model
eval_results = trainer.evaluate()
print("\nBERT Evaluation Results:")
print(eval_results)